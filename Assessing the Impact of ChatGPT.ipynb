{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184ad6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install\n",
    "%pip install nlpaug==1.1.11\n",
    "%pip install imbalanced-learn==0.11.0\n",
    "%pip install openpyxl\n",
    "%pip install wordcloud\n",
    "%pip install contractions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13ee1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaTokenizer, RobertaForSequenceClassification\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e882c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bert Model\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 1. Load and Preprocess Data\n",
    "# =======================\n",
    "df = pd.read_csv(\"/content/Majority Voting 2nd round.csv\")\n",
    "df = df[['Body', 'Majority Vote']].rename(columns={'Body': 'text', 'Majority Vote': 'label'})\n",
    "df = df.dropna()\n",
    "\n",
    "# Get unique labels and their count\n",
    "unique_labels = df['label'].unique()\n",
    "num_labels = len(unique_labels)\n",
    "print(f\"Unique Labels: {unique_labels}, Number of Labels: {num_labels}\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])  # Encode labels to numerical values\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# 2. Tokenization & Dataset Setup\n",
    "# =======================\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length',\n",
    "                                  max_length=self.max_len, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = SentimentDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# =======================\n",
    "# 3. Model and Optimizer\n",
    "# =======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Update num_labels to match the actual number of labels in your dataset\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# =======================\n",
    "# 4. Training Loop\n",
    "# =======================\n",
    "epochs = 2\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# =======================\n",
    "# 5. Evaluation\n",
    "# =======================\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# =======================\n",
    "# 6. Metrics\n",
    "# =======================\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "report = classification_report(all_labels, all_preds, target_names=label_encoder.classes_)\n",
    "\n",
    "print(f\"\\nâœ… Accuracy: {acc:.4f}\")\n",
    "print(\"ðŸ“Š Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba31183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roberta model and tokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Custom Dataset class remains the same\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure text is a string\n",
    "        text = str(self.texts.iloc[idx])  # Convert to string explicitly\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "# Trainer remains the same\n",
    "def compute_metrics(pred):\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# Load the datasets\n",
    "ground_truth_path = '/content/Majority Voting 2nd round.csv'\n",
    "new_data_path = '/content/Academic_data_cleaned.csv'\n",
    "\n",
    "ground_truth = pd.read_csv(ground_truth_path, encoding='latin1')\n",
    "new_data = pd.read_csv(new_data_path)\n",
    "\n",
    "#ground_truth['Body'] = (ground_truth['Title'] + ' ' + ground_truth['Body']).apply(preprocess_text)\n",
    "ground_truth['Body'] = ground_truth['Body'].astype(str)\n",
    "new_data['selftext'] = new_data['selftext'].astype(str)\n",
    "\n",
    "\n",
    "X = ground_truth['Body'].apply(preprocess_text)\n",
    "y = ground_truth['Majority Vote']\n",
    "\n",
    "# Map sentiments to numeric labels\n",
    "label_map = {label: idx for idx, label in enumerate(y.unique())}\n",
    "y = y.map(label_map)\n",
    "\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_map))\n",
    "\n",
    "# Stratified train-test split\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(X, y, tokenizer, max_length=256)\n",
    "test_dataset = TextDataset(new_data['selftext'].apply(preprocess_text), pd.Series([0] * len(new_data)), tokenizer, max_length=256)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "\"\"\"train_dataset = TextDataset(X_train, y_train, tokenizer, max_length=256)  # Increased max_length\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer, max_length=256)\"\"\"\n",
    "\n",
    "# Training arguments remain the same\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,  # Adjust learning rate\n",
    "    per_device_train_batch_size=8,  # Adjust batch size\n",
    "    per_device_eval_batch_size=8,  # Adjust batch size\n",
    "    num_train_epochs=10,  # Adjust number of epochs\n",
    "    weight_decay=0.005,  # Adjust weight decay\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),\n",
    "                               'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n",
    "                               'labels': torch.stack([f['labels'] for f in data])},\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Get predictions on the test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report([0] * len(new_data), pred_labels))\n",
    "accuracy = accuracy_score([0] * len(new_data), pred_labels)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Validation and new dataset predictions remain the same\n",
    "new_data['text'] = (new_data['title'].astype(str) + ' ' + new_data['selftext'].astype(str)).apply(preprocess_text)\n",
    "new_texts = new_data['text']\n",
    "\n",
    "new_encodings = tokenizer(\n",
    "    new_texts.tolist(),\n",
    "    max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=new_encodings['input_ids'],\n",
    "        attention_mask=new_encodings['attention_mask']\n",
    "    )\n",
    "    predictions = torch.argmax(outputs.logits, dim=1).numpy()\n",
    "\n",
    "inverse_label_map = {idx: label for label, idx in label_map.items()}\n",
    "new_data['predicted_sentiment'] = [inverse_label_map[pred] for pred in predictions]\n",
    "\n",
    "output_path = '/content/roBerta_model.csv'\n",
    "new_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Labeled dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63761c48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m ground_truth_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/Validation sets/majority_voting_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m new_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/Reddit_Data/cleaned/Education/merged_output.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(ground_truth_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m new_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(new_data_path)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Prepare the training data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Combine 'Title' and 'Body' for better context\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#Logistic Regression Model\n",
    "\n",
    "# Load the Reddit ground truth dataset\n",
    "ground_truth_path = '/content/drive/MyDrive/Validation sets/majority_voting_results.csv'\n",
    "new_data_path = '/content/drive/MyDrive/Reddit_Data/cleaned/Education/merged_output.csv'\n",
    "\n",
    "ground_truth = pd.read_csv(ground_truth_path, encoding='latin1')\n",
    "new_data = pd.read_csv(new_data_path)\n",
    "\n",
    "# Prepare the training data\n",
    "# Combine 'Title' and 'Body' for better context\n",
    "ground_truth['text'] = ground_truth['Title'] + ' ' + ground_truth['Body']\n",
    "X = ground_truth['text']\n",
    "y = ground_truth['Final Sentimental']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=120)\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and LogisticRegression\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(max_features=5000),\n",
    "    LogisticRegression(random_state=120)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "val_predictions = pipeline.predict(X_val)\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_val, val_predictions))\n",
    "\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Prepare the new dataset\n",
    "# Combine 'title' and 'selftext' for prediction\n",
    "new_data['text'] = new_data['title'] + ' ' + new_data['selftext']\n",
    "\n",
    "# Predict sentiment for the new data\n",
    "new_data['predicted_sentiment'] = pipeline.predict(new_data['text'])\n",
    "\n",
    "# Save the labeled dataset\n",
    "output_path = '/content/drive/MyDrive/Validation sets/labeled_merged_output2.csv'\n",
    "new_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Labeled dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f191786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER Model\n",
    "\n",
    "# ======================\n",
    "# 1. Load and Prepare Data\n",
    "# ======================\n",
    "df = pd.read_csv(\"Majority Voting 2nd round.csv\")\n",
    "df = df[['Body', 'Majority Vote']].rename(columns={'Body': 'text', 'Majority Vote': 'label'})\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode true labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['label'])  # 0 = Negative, 1 = Positive\n",
    "\n",
    "# ======================\n",
    "# 2. Initialize VADER\n",
    "# ======================\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_label(text):\n",
    "    score = analyzer.polarity_scores(text)[\"compound\"]\n",
    "    if score >= 0.8:  # Relaxed positive threshold\n",
    "        return \"Positive\"\n",
    "    elif score <= -0.8:  # Relaxed negative threshold\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# ======================\n",
    "# 3. Apply VADER to Dataset\n",
    "# ======================\n",
    "df['vader_pred'] = df['text'].apply(get_vader_label)\n",
    "\n",
    "# ======================\n",
    "# 4. Evaluation\n",
    "# ======================\n",
    "accuracy = accuracy_score(df['label_encoded'], df['vader_pred'])\n",
    "report = classification_report(df['label_encoded'], df['vader_pred'], target_names=label_encoder.classes_)\n",
    "\n",
    "print(f\"âœ… VADER Accuracy: {accuracy:.4f}\")\n",
    "print(\"ðŸ“Š VADER Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2456c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reddit Cleaner\n",
    "\n",
    "def menu_selection():\n",
    "    \"\"\"\n",
    "    Displays the main menu and handles user input for different actions.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        print(\n",
    "            '\\nMenu Options:'\n",
    "            '\\n1. Scrape Reddit data'\n",
    "            '\\n2. Clean Reddit data'\n",
    "            '\\n3. Exit'\n",
    "        )\n",
    "        selection = input('Enter your choice (1/2/3): ').strip()\n",
    "\n",
    "        if selection == '1':\n",
    "            get_information()\n",
    "        elif selection == '2':\n",
    "            path = input('Enter the path to your data file (e.g., /content/yourfile.csv): ').strip()\n",
    "            clean_data(path)\n",
    "        elif selection == '3':\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        else:\n",
    "            print('Invalid selection. Please try again.')\n",
    "\n",
    "\n",
    "def get_information():\n",
    "    \"\"\"\n",
    "    Collects input from the user to scrape Reddit data and calls the scraper function.\n",
    "    \"\"\"\n",
    "    subreddit = input(\"Enter subreddit(s) to scrape (comma-separated): \").strip()\n",
    "    word = input(\"Enter keyword(s) to search (comma-separated): \").strip()\n",
    "    start = input(\"Enter start date (YYYY-MM-DD): \").strip()\n",
    "    end = input(\"Enter end date (YYYY-MM-DD): \").strip()\n",
    "    folder = input(\"Enter folder to save data (leave blank to save in /content): \").strip()\n",
    "\n",
    "    if not folder:\n",
    "        folder = \"/content\"\n",
    "    else:\n",
    "        folder = os.path.join(\"/content\", folder)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    parse_tag_list(word, subreddit, start, end, folder)\n",
    "\n",
    "\n",
    "def fetch_reddit_posts(subreddit, start_date, end_date, keyword=None, folder=None):\n",
    "    \"\"\"\n",
    "    Fetches Reddit posts for a specific subreddit and keyword within a date range.\n",
    "    :param subreddit: Subreddit name.\n",
    "    :param start_date: Start date in YYYY-MM-DD format.\n",
    "    :param end_date: End date in YYYY-MM-DD format.\n",
    "    :param keyword: Keyword to search.\n",
    "    :param folder: Folder to save the results.\n",
    "    \"\"\"\n",
    "    start_timestamp = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())\n",
    "    end_timestamp = int(datetime.strptime(end_date, '%Y-%m-%d').timestamp())\n",
    "\n",
    "    base_url = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "\n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'after': start_timestamp,\n",
    "        'before': end_timestamp,\n",
    "        'size': 100,\n",
    "        'sort': 'desc',\n",
    "        'sort_type': 'created_utc'\n",
    "    }\n",
    "\n",
    "    if keyword:\n",
    "        params['q'] = keyword\n",
    "\n",
    "    posts = []\n",
    "    while True:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Received status code {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        if 'data' not in data or len(data['data']) == 0:\n",
    "            break\n",
    "\n",
    "        posts.extend(data['data'])\n",
    "        params['before'] = data['data'][-1]['created_utc']\n",
    "\n",
    "    df = pd.DataFrame(posts)\n",
    "    if not df.empty:\n",
    "        df = df[['id', 'title', 'selftext', 'author', 'created_utc', 'url', 'num_comments', 'score']]\n",
    "        df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "        save_path = os.path.join(folder, f\"{subreddit}_{keyword}_{start_date}_{end_date}.csv\")\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Data saved to {save_path}\")\n",
    "\n",
    "\n",
    "def parse_tag_list(tag_list, sub_list, start_date, end_date, folder):\n",
    "    \"\"\"\n",
    "    Processes the list of keywords and subreddits, and calls the fetch function for each combination.\n",
    "    \"\"\"\n",
    "    keywords = [tag.strip() for tag in tag_list.split(',')]\n",
    "    subreddits = [sub.strip() for sub in sub_list.split(',')]\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        for keyword in keywords:\n",
    "            fetch_reddit_posts(subreddit, start_date, end_date, keyword, folder)\n",
    "\n",
    "\n",
    "def clean_data(path):\n",
    "    \"\"\"\n",
    "    Cleans the Reddit data by removing duplicates, empty rows, and posts marked as 'deleted' or 'removed'.\n",
    "    :param path: Path to the CSV file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Error: The file at '{path}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        # Remove duplicate rows\n",
    "        df.drop_duplicates(inplace=True)\n",
    "\n",
    "        # Remove rows with NaN values\n",
    "        #df.dropna(inplace=True)\n",
    "\n",
    "        # Remove rows with 'deleted' or 'removed' in 'selftext'\n",
    "        if 'selftext' in df.columns:\n",
    "            df = df[~df['selftext'].str.contains('deleted|removed', case=False, na=False)]\n",
    "\n",
    "        # Save the cleaned data\n",
    "        save_file = path.replace(\".csv\", \"_cleaned.csv\")\n",
    "        df.to_csv(save_file, index=False)\n",
    "        print(f\"Cleaned data saved to {save_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during cleaning: {e}\")\n",
    "\n",
    "\n",
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    menu_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9454312",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pathlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m concat_csv_in_directory(path, subreddit_domain_file)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m---> 30\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[43mpathlib\u001b[49m\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/Academic_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/unclean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(folder_path)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pathlib' is not defined"
     ]
    }
   ],
   "source": [
    "# Concatenate CSV files in a directory\n",
    "\n",
    "def concat_csv_in_directory(path, subreddit_domain_file):\n",
    "    # Read the main CSV file; if it's empty, initialize an empty DataFrame with expected columns (if any)\n",
    "    try:\n",
    "        df = pd.read_csv(subreddit_domain_file)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "\n",
    "        # Process only CSV files\n",
    "        if os.path.isfile(file_path) and file.endswith(\".csv\"):\n",
    "            try:\n",
    "                df_temp = pd.read_csv(file_path)\n",
    "                df_temp[\"subreddit\"] = file.split('_')[0]  # Add the filename column\n",
    "                df = pd.concat([df, df_temp], ignore_index=True)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"Skipping empty file: {file}\")\n",
    "                continue  # Skip this file if it is empty\n",
    "\n",
    "    df.to_csv(subreddit_domain_file, index=False)\n",
    "    return df\n",
    "\n",
    "def concat_csv(path, subreddit_domain_file):\n",
    "    for file in path.iterdir():\n",
    "        if file.is_file() and file.name.endswith(\".csv\"):\n",
    "            return concat_csv_in_directory(path, subreddit_domain_file)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "file_path = pathlib.Path('/content/Academic_data.csv')\n",
    "folder_path = pathlib.Path('/content/unclean')\n",
    "\n",
    "print(len(os.listdir(folder_path)))\n",
    "\n",
    "if file_path.exists() and file_path.suffix == \".csv\":\n",
    "    print('File found')\n",
    "    df = concat_csv(folder_path, file_path)\n",
    "    print(df)\n",
    "else:\n",
    "    print('File not found')\n",
    "    pd.DataFrame().to_csv(file_path, index=False)  # Create an empty CSV\n",
    "    df = concat_csv(folder_path, file_path)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ded58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment separation\n",
    "\n",
    "# Ask user for the Excel file path\n",
    "file_path = input(\"Enter the full path to your Excel file (e.g., /content/sentiment.xlsx): \")\n",
    "\n",
    "# Read the Excel file\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "except Exception as e:\n",
    "    raise FileNotFoundError(f\"Could not read the file: {e}\")\n",
    "\n",
    "# Ensure required columns exist\n",
    "if not {'Sentiment', 'Body'}.issubset(df.columns):\n",
    "    raise ValueError(\"The Excel file must have 'Sentiment' and 'Body' columns.\")\n",
    "\n",
    "# Normalize sentiment to lowercase\n",
    "df['Sentiment'] = df['Sentiment\n",
    "\n",
    "'].str.lower()\n",
    "\n",
    "# Filter by sentiment\n",
    "positive_df = df[df['Sentiment'] == 'positive'][['Body', 'Sentiment']]\n",
    "negative_df = df[df['Sentiment'] == 'negative'][['Body', 'Sentiment']]\n",
    "neutral_df  = df[df['Sentiment'] == 'neutral'][['Body', 'Sentiment']]\n",
    "\n",
    "# Save to separate Excel files\n",
    "positive_path = 'positive_sentiments.xlsx'\n",
    "negative_path = 'negative_sentiments.xlsx'\n",
    "neutral_path  = 'neutral_sentiments.xlsx'\n",
    "\n",
    "positive_df.to_excel(positive_path, index=False)\n",
    "negative_df.to_excel(negative_path, index=False)\n",
    "neutral_df.to_excel(neutral_path, index=False)\n",
    "\n",
    "print(\"\\nâœ… Files saved successfully:\")\n",
    "print(f\"- {positive_path}\")\n",
    "print(f\"- {negative_path}\")\n",
    "print(f\"- {neutral_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc90f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement Score Calculation\n",
    "\n",
    "def calculate_agreement_score():\n",
    "    # Prompt the user to enter the file paths for each file\n",
    "    majority_file = input(\"Enter the path of the Majority Voting result file: \")\n",
    "    comparison_file = input(\"Enter the path of the comparison file: \")\n",
    "\n",
    "    try:\n",
    "        # Load the two Excel files\n",
    "        majority_df = pd.read_excel(majority_file)\n",
    "        comparison_df = pd.read_excel(comparison_file)\n",
    "\n",
    "        # Check if the 'Majority Vote' column exists in the majority file\n",
    "        if 'Majority Vote' not in majority_df.columns:\n",
    "            print(\"Error: 'Majority Vote' column not found in the majority voting file.\")\n",
    "            return None\n",
    "\n",
    "        # Check if the 'Sentiment' column exists in the comparison file\n",
    "        if 'Sentiment' not in comparison_df.columns:\n",
    "            print(\"Error: 'new Sentiment' column not found in the comparison file.\")\n",
    "            return None\n",
    "\n",
    "        # Make sure both files have the same number of rows for comparison\n",
    "        if len(majority_df) != len(comparison_df):\n",
    "            print(\"Error: Files do not have the same number of rows.\")\n",
    "            return None\n",
    "\n",
    "        # Capitalize the values in both columns for consistency\n",
    "        majority_df['Majority Vote'] = majority_df['Majority Vote'].str.capitalize()\n",
    "        comparison_df['Sentiment'] = comparison_df['Sentiment'].str.capitalize()\n",
    "\n",
    "        # Calculate the agreement (count how many rows are the same in both columns)\n",
    "        agreements = (majority_df['Majority Vote'] == comparison_df['Sentiment']).sum()\n",
    "        total = len(majority_df)\n",
    "        agreement_score = (agreements / total) * 100\n",
    "\n",
    "        # Output the agreement score\n",
    "        print(f\"\\nAgreement Score: {agreement_score:.2f}%\")\n",
    "        return agreement_score\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Run the function to calculate the agreement score\n",
    "calculate_agreement_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa10ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reddit Scraper\n",
    "\n",
    "def menu_selection():\n",
    "  print('Input a number from the menu below:'\n",
    "          '\\n1. Scrape Reddit data'\n",
    "          '\\n2. clean Reddit data'\n",
    "          '\\n3. Exit')\n",
    "\n",
    "  selection = input('Enter your choice: ')\n",
    "\n",
    "  if selection == '1':\n",
    "      get_information()\n",
    "\n",
    "  if selection == '2':\n",
    "      path = input('Enter the path to your data: ')\n",
    "      save_path = input('Enter the path to save your data please include .csv: ')\n",
    "      clean_data('./' + path, save_path)\n",
    "\n",
    "  if selection == '3':\n",
    "      return None\n",
    "\n",
    "  elif selection != '1' or selection != '2' or selection != '3':\n",
    "    print('Invalid selection, try again')\n",
    "    menu_selection()\n",
    "\n",
    "def get_information():\n",
    "    subreddit = input(\"Enter the name of the subreddit to scrape, if multiple separate with a comma: \")\n",
    "    word = input(\"Enter the word you'd like to search, if multiple separate each with a comma: \")\n",
    "    start = input(\"Enter the date to begin scraping YYYY-MM-DD: \")\n",
    "    end = input(\"Enter the date to end scraping YYYY-MM-DD: \")\n",
    "    folder = input(\"Enter the folder where you want the data to be saved: \")\n",
    "    parse_tag_list(word, subreddit, start, end, folder)\n",
    "\n",
    "\"\"\"#Scraping function\"\"\"\n",
    "\n",
    "def fetch_reddit_posts(subreddit, start_date, end_date, keyword=None, folder=None):\n",
    "    \"\"\"\n",
    "    Fetches Reddit posts from a given subreddit.\n",
    "    :param folder:\n",
    "    :param subreddit:  The subreddit to fetch posts from.\n",
    "    :param start_date:  The start date to retrieve posts from.\n",
    "    :param end_date:  The end date to retrieve posts from.\n",
    "    :param keyword:  The keyword used to fetch posts from.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert dates to Unix timestamps\n",
    "    start_timestamp = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())\n",
    "    end_timestamp = int(datetime.strptime(end_date, '%Y-%m-%d').timestamp())\n",
    "\n",
    "    base_url = 'https://api.pullpush.io/reddit/search/submission/'\n",
    "\n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'after': start_timestamp,\n",
    "        'before': end_timestamp,\n",
    "        'size': 100,  # Number of results per request (max 100)\n",
    "        'sort': 'desc',\n",
    "        'sort_type': 'created_utc'\n",
    "    }\n",
    "\n",
    "    if keyword:\n",
    "        params['q'] = keyword\n",
    "\n",
    "    posts = []\n",
    "    while True:\n",
    "        print(f\"Fetching data with params: {params}\")\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Received status code {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        if 'data' not in data or len(data['data']) == 0:\n",
    "            print(\"No data found or no more posts to fetch.\")\n",
    "            break\n",
    "\n",
    "        posts.extend(data['data'])\n",
    "\n",
    "        print(f\"Fetched {len(data['data'])} posts.\")\n",
    "\n",
    "        # Update the 'before' parameter to the timestamp of the last fetched post\n",
    "        params['before'] = data['data'][-1]['created_utc']\n",
    "\n",
    "    # Create a DataFrame from the posts\n",
    "    df = pd.DataFrame(posts)\n",
    "\n",
    "    # Select relevant columns\n",
    "    if not df.empty:\n",
    "        df = df[['id', 'title', 'selftext', 'author', 'created_utc', 'url', 'num_comments', 'score']]\n",
    "\n",
    "        # Convert the created_utc column to a readable date format\n",
    "        df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "        # Save DataFrame to CSV if a file path is provided\n",
    "        if not folder:\n",
    "          df.to_csv(f\"{subreddit}_{keyword}_{start_date}_{end_date}.csv\", index=False)\n",
    "          print(f\"Saved to {subreddit}_{keyword}_{start_date}_{end_date}.csv\")\n",
    "\n",
    "        if folder and os.path.exists(folder):\n",
    "          print('folder true and path exist')\n",
    "          df.to_csv(f\"{folder}/{subreddit}_{keyword}_{start_date}_{end_date}.csv\", index=False)\n",
    "          print(f\"Saved to {folder}/{subreddit}_{keyword}_{start_date}_{end_date}.csv\")\n",
    "\n",
    "        elif folder and not os.path.exists(folder):\n",
    "          print('folder true but doesnt exist')\n",
    "          try:\n",
    "            os.mkdir(folder)\n",
    "            df.to_csv(f\"{folder}/{subreddit}_{keyword}_{start_date}_{end_date}.csv\", index=False)\n",
    "            print(f\"Saved to {folder}/{subreddit}_{keyword}_{start_date}_{end_date}.csv\")\n",
    "          except:\n",
    "            print('An error occured')\n",
    "    return df\n",
    "\n",
    "\"\"\"#Function that checks for multiple tags and subreddits\"\"\"\n",
    "\n",
    "def parse_tag_list(tag_list, sub_list, start_date, end_date, folder):\n",
    "    keyword_list = tag_list.split(',')\n",
    "    subreddit_list = sub_list.split(',')\n",
    "\n",
    "    for x in range(0, len(subreddit_list)):\n",
    "      subreddit_list[x] = subreddit_list[x].strip()\n",
    "\n",
    "    for x in range(0, len(keyword_list)):\n",
    "      keyword_list[x] = keyword_list[x].strip()\n",
    "\n",
    "    for subreddit in subreddit_list:\n",
    "\n",
    "        for word in keyword_list:\n",
    "            fetch_reddit_posts(subreddit, start_date, end_date, word, folder)\n",
    "\n",
    "\n",
    "\"\"\"#Function to clean data\"\"\"\n",
    "\n",
    "def clean_data(path, save_file):\n",
    "    df = pd.DataFrame({})\n",
    "    # If path is directory concat all data\n",
    "    if os.path.isdir(path):\n",
    "        for file in os.listdir(path):\n",
    "            df_temp = pd.read_csv(os.path.join(path, file))\n",
    "            df = pd.concat([df, df_temp], ignore_index=True)\n",
    "\n",
    "    if not os.path.isdir(path):\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "    # Remove duplicate data points\n",
    "    df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "    # Remove empty data\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Save df to new CSV\n",
    "    df.to_csv(save_file, index=False)\n",
    "\n",
    "\"\"\"#Call function to scrape data\"\"\"\n",
    "\n",
    "menu_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b61dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud\n",
    "\n",
    "folder_path = \"/content/drive/MyDrive/Reddit_Data/GradSchool_chatgpt_2023-1-1_2023-4-30.csv\"  # Adjust this to point to the 'reddit' folder on your flash drive\n",
    "file_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Set the stopwords list\n",
    "stopwords = set(STOPWORDS)\n",
    "new_words = [ 'https','i', 'you', 'me', 'a', 'us','thank','you','chatgpt','ve','lot','please','now','something']  # Add other words you want to exclude\n",
    "new_stopwords = stopwords.union(new_words)\n",
    "\n",
    "# df = pd.read_csv(files)\n",
    "if files:\n",
    "    # Read the first (and presumably only) CSV file\n",
    "    df = pd.read_csv(files)\n",
    "else:\n",
    "    print(\"No CSV files found in the specified folder.\")\n",
    "    # Handle the case where no files are found, e.g., exit or raise an exception\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "\n",
    "if text.strip():  # Check if text is not empty or just whitespace\n",
    "    wordcloud = WordCloud(\n",
    "        max_font_size=50,\n",
    "        max_words=100,\n",
    "        background_color=\"white\",\n",
    "        stopwords=new_stopwords,\n",
    "        colormap='Dark2'  # Choose a contrasting colormap\n",
    "    ).generate(text)\n",
    "\n",
    "    # Plot Wordcloud\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No text available to generate word cloud.\")\n",
    "\n",
    "if text.strip():  # Check if text is not empty or just whitespace\n",
    "    wordcloud = WordCloud(\n",
    "        max_font_size=50,\n",
    "        max_words=100,\n",
    "        background_color=\"white\",\n",
    "        stopwords=new_stopwords,\n",
    "        colormap='Dark2'  # Choose a contrasting colormap\n",
    "    ).generate(text)\n",
    "\n",
    "    # Plot Wordcloud\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No text available to generate word cloud.\")\n",
    "\n",
    "# Size of Word Cloud\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "# Make Wordcloud\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\",stopwords=new_stopwords, colormap='flag').generate(text)\n",
    "\n",
    "# Plot Wordcloud\n",
    "plt.plot()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c037d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "def remove_stopwords(text) -> str:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def standardize_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "# Load CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Validation sets/Marcos' Validation dataset - Sheet1.csv\")\n",
    "\n",
    "# download stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(df['Body'])\n",
    "\n",
    "contracted_string = df['Body'].apply(lambda text: contractions.fix(text))\n",
    "#print(contracted_string)\n",
    "\n",
    "lemmatized_string = contracted_string.apply(lemmatize_text)\n",
    "#print(lemmatized_string)\n",
    "\n",
    "df['Body'] = lemmatized_string.apply(standardize_text)\n",
    "\n",
    "no_stpwords_string = df['Body'].apply(remove_stopwords)\n",
    "#print(no_stpwords_string)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(no_stpwords_string)\n",
    "print(tfidf_vectors.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
